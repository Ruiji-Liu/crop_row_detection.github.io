<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VIMA | General Robot Manipulation with Multimodal Prompts</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based Crop-Row Detection in Arable Fields</h1>
                    <h3 class="title is-4 conference-authors"><a target="_blank" href="https://icml.cc/">ICRA 2025 (under reivew)</a>
                    </h3>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://sites.google.com/andrew.cmu.edu/ruiji-liu">Ruiji&#160;Liu</a><sup>1;</sup>,
                <a target="_blank"
                   href="https://www.ri.cmu.edu/ri-people/francisco-yandun/">Francisco&#160;Yandun</a><sup>1;</sup>,
                <a target="_blank" href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/">George&#160;Kantor</a><sup>1</sup>,
                <br>
            </span>
                    </div>

                    <div class="image-container has-text-centered">
                        <img src="images/ri.png" alt="Figure 1", style="width: 300px; height: 70px">
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2403.17774"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                            <span class="link-block">
                <a target="_blank" href="assets/crop_row_detection.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/Kantor-Lab/LiDAR_CropRowDetection"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://youtu.be/FYJuxgDMiHE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="background-color: #ffffff;">
    <div class="hero-body">
        <div class="container">
            
            <div style="display: flex; gap: 10px;">
                <div class="item item-sweep_without_exceeding">
                    <video poster="" id="sweep_without_exceeding" autoplay controls muted loop height="70%" playbackRate="2.0">
                        <source src="assets/videos/visualization.mp4" type="video/mp4">
                    </video>
                </div>
                
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        Autonomous navigation is crucial for various robotics applications in agriculture. However, many existing methods depend on RTK-GPS devices, 
                        which can be susceptible to loss of radio signal or intermittent reception of corrections from the internet. Consequently, research has increasingly 
                        focused on using RGB cameras for crop-row detection, though challenges persist when dealing with grown plants. This paper introduces a LiDAR-based 
                        navigation system that can achieve crop-agnostic over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the inter-row 
                        spacing. Our algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, the presence of weeds, 
                        curved rows, and discontinuities. Without utilizing a global localization method (i.e., based on GPS), our navigation system can perform 
                        autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, 
                        providing a crop-agnostic approach to navigate an entire field. The proposed navigation system has undergone tests in various simulated and 
                        real agricultural fields, achieving an average cross-track error of 3.55cm without human intervention. The system has been deployed on a 
                        customized UGV robot, which can be reconfigured depending on the field conditions.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <img src="assets/images/system.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px;margin-top: 20px; width: 80%;"/>
                    <br>
                    <span style="font-size: 110%"><b>Navigation system’s workflow.</b> (i) The crop row detection algorithm uses LiDAR data and filtered odometry values [x, y, ψ] as inputs, predicting
                        crop rows in the form [x1, y1, x2, y2] within the robot’s frame. (ii) The crop row following algorithm applies nonlinear MPC to control the robot to follow
                        the center line of the predicted rows, sending linear velocity v and angular velocity w commands. (iii) The crop row switching algorithm utilizes a PID
                        controller to navigate the robot to the next lane if no more crop rows are detected.</span>
                    </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">1. Crop Row Detection</span></h2>
                    <div class="row is-full-width">
                        <img src="assets/images/detection.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br></div>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Our crop-row detection algorithm is comprised of three
                            key components. First, we estimate the ground plane using
                            the LiDAR’s tilted angle θ, raise it to intersect the point
                            cloud centroid, and filter out points below this plane. This
                            process isolates the returns corresponding only to the top of
                            the plants. Second, employing this filtered LiDAR data, we
                            apply the K-means clustering algorithm to segment crop rows
                            autonomously. The generated centroids of these segments
                            represent the center of crop rows. We utilize the robot’s
                            filtered odometry, [x, y, ψ], to accumulate detected centroids
                            into the robot frame within a short time window. This
                            approach allows us to have accurate local positioning and
                            avoid drifting. Finally, we implement the RANSAC line
                            fitting algorithm on this crop-row centroids map, extracting
                            2D line locations for the first row on the left and the first
                            row on the right in the robot frame.
                        </p>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">2. Crop Row Following</span></h2>
                    <div class="row is-full-width">
                        <img src="assets/images/mpc.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br></div>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            After crop-row detection, we generate waypoints along the center line of the two predicted crop rows. 
                            We apply a nonlinear Model Predictive Control (MPC) algorithm in the robot's local frame for tracking the generated waypoints. 
                            <a href="https://acado.github.io/" target="_blank" style="color: #1e90ff; text-decoration: underline;">ACADO</a> is used to solve the quadratic programming problem, enabling real-time operation
                        </p>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">3. Crop Row Switching</span></h2>
                    <div class="row is-full-width">
                        <video poster="" autoplay controls muted loop height="100%">
                        <source src="assets/videos/switching.mp4"
                                type="video/mp4">
                        </video>
                            <br>
                            <span style="font-size: 110%"><b>Whole field coverage strategy.</b>  We implemented a PID controller method that uses mainly filtered odometry data.
                                Since the crops are typically planted in parallel rows for efficient management, this approach first rotates the robot by 90 degrees, 
                                drives it forward based on the distance between crop rows, and then performs another 90-degree turn to navigate into the next lane. 
                                Then, the LiDAR-based navigation system is started again and navigates the robot through the new lane. 
                                This process is implemented as a finite state machine, which enables the automatic change of states during execution.</span>
                            </div>
                </div>
            </div>

        </div>
    </div>
</section>
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">3. Experiments</span></h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            We conducted experiments in both Gazebo simulated environments and real fields with different crops (corn & soybean) and growth 
                            stages (young & grown) to test our crop-row detection and crop-row following algorithms performance. 
                        </p>
                    </div>
                    <div class="row is-full-width">
                        <img src="assets/images/drone.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br>
                            <span style="font-size: 110%"><b>Fields overview and the collected drone maps.</b>The experiments are conducted in soybean (top left), 
                                corn (top middle), and curved corn (top right) fields. We collected the drone map for (soybean (bottom left), corn (bottom middle), 
                                and curved corn (bottom right)) with RTK-GPS inputs as ground truth for later evaluations. We overlay the detected centroids on the drone 
                                map to provide qualitative results, demonstrating that the detected centroids align with the crop rows as shown in the map.</span>
                    </div>
                    
                    <div class="row is-full-width">
                        <img src="assets/images/results.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 100%;"/>
                                 
                            <br>
                            <span style="font-size: 110%"><b>Navigation system’s workflow.</b> (i) The crop row detection algorithm uses LiDAR data and filtered odometry values [x, y, ψ] as inputs, predicting
                                crop rows in the form [x1, y1, x2, y2] within the robot’s frame. (ii) The crop row following algorithm applies nonlinear MPC to control the robot to follow
                                the center line of the predicted rows, sending linear velocity v and angular velocity w commands. (iii) The crop row switching algorithm utilizes a PID
                                controller to navigate the robot to the next lane if no more crop rows are detected.</span>

                    </div>

                    <div class="row is-full-width">
                        <img src="assets/images/switching.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 50%;"/>
                                 
                            <br>
                            <span style="font-size: 110%"><b>Trajectory</b> (red) of the robot while navigating across the whole field (green) in Gazebo-simulated young soybean (top left), 
                                young corn (bottom left), grown soybean (top right), and grown corn (bottom right) fields (30m × 12m with 16 rows). These show our crop-row switching maneuver
                            ability to cover the whole fields with different crop types and different crop stages</span>

                    </div>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            
                        </p>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
                        In this paper, we present a novel LiDAR-based crop-row detection approach that integrates the Model Predictive Control (MPC) 
                        and lane-switching algorithm to create an autonomous navigation system for agricultural robots in row-crop fields. 
                        This system facilitates independent robot navigation for diverse agricultural tasks, contributing to precision farming. 
                        Our crop-row detection method utilizes 3D LiDAR data to extract the height information and accurately detects crop rows amidst 
                        challenging scenarios such as canopy obstructions. The whole navigation system incorporates the crop-row detection, following, 
                        and switching algorithm, enabling automated tracking of detected crop rows and full field coverage. 
                        This navigation system is evaluated in both actual fields and Gazebo simulated fields with a 1:1 scale Amiga robot model. 
                        The crop-row detection algorithm achieves an average detection accuracy of 3.35cm, while the crop-row following algorithm 
                        achieves an average driving accuracy of 3.55cm. Future work will focus on improving the robustness of the crop row perception 
                        algorithm by integrating camera data, especially to handle gaps between plants during the germination stage.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{liu2024overcanopyautonomousnavigationcropagnostic,
            title={Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based Crop-Row Detection in Arable Fields}, 
            author={Ruiji Liu and Francisco Yandun and George Kantor},
            year={2024},
            eprint={2403.17774},
            archivePrefix={arXiv},
            primaryClass={cs.RO},
            url={https://arxiv.org/abs/2403.17774}, 
      }</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                            href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
