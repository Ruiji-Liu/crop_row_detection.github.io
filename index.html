<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>VIMA | General Robot Manipulation with Multimodal Prompts</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based Crop-Row Detection in Arable Fields</h1>
                    <h3 class="title is-4 conference-authors"><a target="_blank" href="https://icml.cc/">ICRA 2025 (under reivew)</a>
                    </h3>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://sites.google.com/andrew.cmu.edu/ruiji-liu">Ruiji&#160;Liu</a><sup>1;</sup>,
                <a target="_blank"
                   href="https://www.ri.cmu.edu/ri-people/francisco-yandun/">Francisco&#160;Yandun</a><sup>1;</sup>,
                <a target="_blank" href="https://www.ri.cmu.edu/ri-faculty/george-a-kantor/">George&#160;Kantor</a><sup>1</sup>,
                <br>
            </span>
                    </div>

                    <div class="image-container has-text-centered">
                        <img src="images/ri.png" alt="Figure 1", style="width: 300px; height: 70px">
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2403.17774"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                            <span class="link-block">
                <a target="_blank" href="assets/crop_row_detection.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/Kantor-Lab/LiDAR_CropRowDetection"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                <a target="_blank" href="https://youtu.be/FYJuxgDMiHE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" style="background-color: #ffffff;">
    <div class="hero-body">
        <div class="container">
            
            <div style="display: flex; gap: 10px;">
                <div class="item item-sweep_without_exceeding">
                    <video poster="" id="sweep_without_exceeding" autoplay controls muted loop height="70%" playbackRate="2.0">
                        <source src="assets/videos/visualization.mp4" type="video/mp4">
                    </video>
                </div>
                
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        Autonomous navigation is crucial for various robotics applications in agriculture. However, many existing methods depend on RTK-GPS devices, 
                        which can be susceptible to loss of radio signal or intermittent reception of corrections from the internet. Consequently, research has increasingly 
                        focused on using RGB cameras for crop-row detection, though challenges persist when dealing with grown plants. This paper introduces a LiDAR-based 
                        navigation system that can achieve crop-agnostic over-canopy autonomous navigation in row-crop fields, even when the canopy fully blocks the inter-row 
                        spacing. Our algorithm can detect crop rows across diverse scenarios, encompassing various crop types, growth stages, the presence of weeds, 
                        curved rows, and discontinuities. Without utilizing a global localization method (i.e., based on GPS), our navigation system can perform 
                        autonomous navigation in these challenging scenarios, detect the end of the crop rows, and navigate to the next crop row autonomously, 
                        providing a crop-agnostic approach to navigate an entire field. The proposed navigation system has undergone tests in various simulated and 
                        real agricultural fields, achieving an average cross-track error of 3.55cm without human intervention. The system has been deployed on a 
                        customized UGV robot, which can be reconfigured depending on the field conditions.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <img src="assets/images/system.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px;margin-top: 20px; width: 80%;"/>
                    <br>
                    <span style="font-size: 110%"><b>Navigation system’s workflow.</b> (i) The crop row detection algorithm uses LiDAR data and filtered odometry values [x, y, ψ] as inputs, predicting
                        crop rows in the form [x1, y1, x2, y2] within the robot’s frame. (ii) The crop row following algorithm applies nonlinear MPC to control the robot to follow
                        the center line of the predicted rows, sending linear velocity v and angular velocity w commands. (iii) The crop row switching algorithm utilizes a PID
                        controller to navigate the robot to the next lane if no more crop rows are detected.</span>
                    </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">1. Crop Row Detection</span></h2>
                    <div class="row is-full-width">
                        <img src="assets/images/detection.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br></div>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Our crop-row detection algorithm is comprised of three
                            key components. First, we estimate the ground plane using
                            the LiDAR’s tilted angle θ, raise it to intersect the point
                            cloud centroid, and filter out points below this plane. This
                            process isolates the returns corresponding only to the top of
                            the plants. Second, employing this filtered LiDAR data, we
                            apply the K-means clustering algorithm to segment crop rows
                            autonomously. The generated centroids of these segments
                            represent the center of crop rows. We utilize the robot’s
                            filtered odometry, [x, y, ψ], to accumulate detected centroids
                            into the robot frame within a short time window. This
                            approach allows us to have accurate local positioning and
                            avoid drifting. Finally, we implement the RANSAC line
                            fitting algorithm on this crop-row centroids map, extracting
                            2D line locations for the first row on the left and the first
                            row on the right in the robot frame.
                        </p>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">2. Crop Row Following</span></h2>
                    <div class="row is-full-width">
                        <img src="assets/images/mpc.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br></div>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            After crop-row detection, we generate waypoints along the center line of the two predicted crop rows. 
                            We apply a nonlinear Model Predictive Control (MPC) algorithm in the robot's local frame for tracking the generated waypoints. 
                            <a href="https://acado.github.io/" target="_blank" style="color: #1e90ff; text-decoration: underline;">ACADO</a> is used to solve the quadratic programming problem, enabling real-time operation
                        </p>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">3. Experiments</span></h2>
                    <div class="row is-full-width">
                        <img src="assets/images/drone.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br>
                            <span style="font-size: 110%"><b>Navigation system’s workflow.</b> (i) The crop row detection algorithm uses LiDAR data and filtered odometry values [x, y, ψ] as inputs, predicting
                                crop rows in the form [x1, y1, x2, y2] within the robot’s frame. (ii) The crop row following algorithm applies nonlinear MPC to control the robot to follow
                                the center line of the predicted rows, sending linear velocity v and angular velocity w commands. (iii) The crop row switching algorithm utilizes a PID
                                controller to navigate the robot to the next lane if no more crop rows are detected.</span>
                    </div>
                    <div class="row is-full-width">
                        <img src="assets/images/drone.png" class="interpolation-image"
                                 alt="" style="display: block; margin-left: auto; margin-right: auto;margin-bottom: 20px; margin-top: 20px;width: 80%;"/>
                            <br>
                            <span style="font-size: 110%"><b>Navigation system’s workflow.</b> (i) The crop row detection algorithm uses LiDAR data and filtered odometry values [x, y, ψ] as inputs, predicting
                                crop rows in the form [x1, y1, x2, y2] within the robot’s frame. (ii) The crop row following algorithm applies nonlinear MPC to control the robot to follow
                                the center line of the predicted rows, sending linear velocity v and angular velocity w commands. (iii) The crop row switching algorithm utilizes a PID
                                controller to navigate the robot to the next lane if no more crop rows are detected.</span>
                    </div>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            
                        </p>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>
<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">VIMA: Visuomotor Attention Agent</span></h2>
                    <video poster="" autoplay controls muted loop height="100%">
                        <source src="assets/videos/vima_arch_animation.mp4"
                                type="video/mp4">
                    </video>
                    <br>
                    <span style="font-size: 110%">
<span style="font-weight: bold">VIMA architecture.</span> We encode the multimodal prompts with a pre-trained T5 model, and condition the robot controller on the prompt through cross-attention layers. The controller is a causal transformer decoder consisting of alternating self and cross attention layers that predicts motor commands conditioned on prompts and interaction history.</span>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">

        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">VIMA-Bench: Benchmark for Multimodal Robot Learning</span></h2>
                    <span style="font-size: 125%">
                        We provide 17 representative tasks with multimodal prompt templates, which can be procedurally instantiated into thousands of individual instances by various combinations of textures and tabletop objects.
                    </span>

                    <br>
                    <br>
                    <br>

                    <div class="columns">
                        <!-- Simple Object Manipulation -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Simple Object Manipulation</h3>
                            <div class="select is-medium">
                                <select id="simple-object-manipulation-menu-tasks"
                                        onchange="updateDemoVideo('simple-object-manipulation')">
                                    <option value="simple_manipulation" selected="selected">Visual Manipulation</option>
                                    <option value="rotate">Rotate</option>
                                    <option value="scene_understanding">Scene Understanding</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="simple-object-manipulation-menu-instances"
                                        onchange="updateDemoVideo('simple-object-manipulation')">
                                    <option value="1" selected="selected">1</option>
                                    <option value="2">2</option>
                                    <option value="3">3</option>
                                    <option value="4">4</option>
                                    <option value="5">5</option>
                                </select>
                            </div>
                            <video id="simple-object-manipulation-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="assets/videos/demos/simple_object_manipulation/simple_manipulation/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>

                        <!-- Visual Goal Reaching -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Visual Goal Reaching</h3>
                            <div class="select is-medium">
                                <select id="visual-goal-reaching-menu-tasks"
                                        onchange="updateDemoVideo('visual-goal-reaching')">
                                    <option value="rearrange" selected="selected">Rearrange</option>
                                    <option value="rearrange_then_restore">Rearrange Then Restore</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="visual-goal-reaching-menu-instances"
                                        onchange="updateDemoVideo('visual-goal-reaching')">
                                    <option value="1" selected="selected">1</option>
                                    <option value="2">2</option>
                                    <option value="3">3</option>
                                    <option value="4">4</option>
                                    <option value="5">5</option>
                                </select>
                            </div>
                            <video id="visual-goal-reaching-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="assets/videos/demos/visual_goal_reaching/rearrange/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>


                    </div>

                    <br>
                    <div class="columns">
                        <!-- Novel Concept Grounding -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Novel Concept Grounding</h3>
                            <div class="select is-medium">
                                <select id="novel-concept-grounding-menu-tasks"
                                        onchange="updateDemoVideo('novel-concept-grounding')">
                                    <option value="novel_adj_and_noun" selected="selected">Novel Adjective and Noun
                                    </option>
                                    <option value="novel_adj">Novel Adjective</option>
                                    <option value="novel_noun">Novel Noun</option>
                                    <option value="twist">Twist</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="novel-concept-grounding-menu-instances"
                                        onchange="updateDemoVideo('novel-concept-grounding')">
                                    <option value="1" selected="selected">1</option>
                                    <option value="2">2</option>
                                    <option value="3">3</option>
                                    <option value="4">4</option>
                                    <option value="5">5</option>
                                </select>
                            </div>
                            <video id="novel-concept-grounding-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="assets/videos/demos/novel_concept_grounding/novel_adj_and_noun/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>

                        <!-- One Shot Video Imitation -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">One-shot Video Imitation</h3>
                            <div class="select is-medium">
                                <select id="one-shot-video-imitation-menu-tasks"
                                        onchange="updateDemoVideo('one-shot-video-imitation')">
                                    <option value="follow_motion" selected="selected">Follow Motion</option>
                                    <option value="follow_order">Follow Order</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="one-shot-video-imitation-menu-instances"
                                        onchange="updateDemoVideo('one-shot-video-imitation')">
                                    <option value="1" selected="selected">1</option>
                                    <option value="2">2</option>
                                    <option value="3">3</option>
                                    <option value="4">4</option>
                                    <option value="5">5</option>
                                </select>
                            </div>
                            <video id="one-shot-video-imitation-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="assets/videos/demos/one_shot_video_imitation/follow_motion/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>
                    </div>

                    <br>
                    <div class="columns">
                        <!-- Visual Constraint Satisfaction -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Visual Constraint Satisfaction</h3>
                            <div class="select is-medium">
                                <select id="visual-constraint-satisfaction-menu-tasks"
                                        onchange="updateDemoVideo('visual-constraint-satisfaction')">
                                    <option value="sweep_without_exceeding" selected="selected">Sweep without
                                        Exceeding
                                    </option>
                                    <option value="sweep_without_touching">Sweep without Touching</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="visual-constraint-satisfaction-menu-instances"
                                        onchange="updateDemoVideo('visual-constraint-satisfaction')">
                                    <option value="1" selected="selected">1</option>
                                    <option value="2">2</option>
                                    <option value="3">3</option>
                                    <option value="4">4</option>
                                    <option value="5">5</option>
                                </select>
                            </div>
                            <video id="visual-constraint-satisfaction-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   playbackRate="2.0"
                                   width="100%">
                                <source src="assets/videos/demos/visual_constraint_satisfaction/sweep_without_exceeding/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>

                        <!-- Visual Reasoning -->
                        <div class="column has-text-left">
                            <h3 class="title is-5">Visual Reasoning</h3>
                            <div class="select is-medium">
                                <select id="visual-reasoning-menu-tasks" onchange="updateDemoVideo('visual-reasoning')">
                                    <option value="manipulate_old_neighbor" selected="selected">Manipulate Old
                                        Neighbor
                                    </option>
                                    <option value="pick_in_order_then_restore">Pick in Order Then Restore</option>
                                    <option value="same_color">Same Texture</option>
                                    <option value="same_profile">Same Shape</option>
                                </select>
                            </div>
                            <div class="select is-medium">
                                <select id="visual-reasoning-menu-instances"
                                        onchange="updateDemoVideo('visual-reasoning')">
                                    <option value="1" selected="selected">1</option>
                                    <option value="2">2</option>
                                    <option value="3">3</option>
                                    <option value="4">4</option>
                                    <option value="5">5</option>
                                </select>
                            </div>
                            <video id="visual-reasoning-single-task-video"
                                   controls
                                   muted
                                   autoplay
                                   loop
                                   width="100%">
                                <source src="assets/videos/demos/visual_reasoning/manipulate_old_neighbor/1.mp4"
                                        type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Experiments</span></h2>

                    <p style="font-size: 125%">
                        We answer three main questions during experiments:
                    <ul style="font-size: 125%; padding-left: 5%">
                        <li>
                            1. What is the best recipe for building multi-task transformer-based robot agents with
                            multimodal prompts?
                        </li>
                        <li>
                            2. What are the <span style="font-weight: bold">scaling properties</span> of our approach in
                            model capacity and data size?
                        </li>
                        <li>
                            3. How do different components, such as visual tokenizers, prompt conditioning, and prompt
                            encoding, affect robot performance?
                        </li>

                    </ul>
                    </p>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Evaluation Results</span></h3>

                    <img src="assets/images/scalability.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Scaling model and data.</span> <i>Top</i>: We compare performance of different methods with model sizes ranging from 2M to 200M parameters. Across all model sizes and generalization levels, VIMA outperforms baseline variants. <i>Bottom</i>: For a fixed model size of 92M parameters we compare the effect of imitation learning dataset size with 0.1%, 1%, 10%, and full data. VIMA is extremely sample efficient and can achieve performance comparable to other methods with 10x less data.
                    </span>
                    <br>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Ablation Studies </span></h3>

                    <br>

                    <img src="assets/images/ablation_input_process.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on visual tokenizers.</span> We compare the performance of VIMA-200M model across different visual tokenizers. Our proposed object tokens outperform all methods that learn directly from raw pixels, and <i>Object Perceiver</i> that downsamples the object sequence to a fixed number of tokens.
                    </span>

                    <br>
                    <br>
                    <br>
                    <br>

                    <img src="assets/images/global_seq_mod.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on prompt conditioning.</span> We compare our method (<i>xattn</i>: cross-attention prompt conditioning) with a vanilla transformer decoder (<i>gpt-decoder</i>) across different model sizes. Cross-attention is especially helpful in low-parameter regime and for harder generalization tasks.
                    </span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
                        In this paper, we present a novel LiDAR-based crop-row detection approach that integrates the Model Predictive Control (MPC) 
                        and lane-switching algorithm to create an autonomous navigation system for agricultural robots in row-crop fields. 
                        This system facilitates independent robot navigation for diverse agricultural tasks, contributing to precision farming. 
                        Our crop-row detection method utilizes 3D LiDAR data to extract the height information and accurately detects crop rows amidst 
                        challenging scenarios such as canopy obstructions. The whole navigation system incorporates the crop-row detection, following, 
                        and switching algorithm, enabling automated tracking of detected crop rows and full field coverage. 
                        This navigation system is evaluated in both actual fields and Gazebo simulated fields with a 1:1 scale Amiga robot model. 
                        The crop-row detection algorithm achieves an average detection accuracy of $3.35cm$, while the crop-row following algorithm 
                        achieves an average driving accuracy of $3.55cm$. Future work will focus on improving the robustness of the crop row perception 
                        algorithm by integrating camera data, especially to handle gaps between plants during the germination stage.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{liu2024overcanopyautonomousnavigationcropagnostic,
            title={Towards Over-Canopy Autonomous Navigation: Crop-Agnostic LiDAR-Based Crop-Row Detection in Arable Fields}, 
            author={Ruiji Liu and Francisco Yandun and George Kantor},
            year={2024},
            eprint={2403.17774},
            archivePrefix={arXiv},
            primaryClass={cs.RO},
            url={https://arxiv.org/abs/2403.17774}, 
      }</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                            href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>